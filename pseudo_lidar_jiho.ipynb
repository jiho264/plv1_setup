{"cells":[{"cell_type":"markdown","metadata":{"id":"fGvmKbOMA4o2"},"source":["# <Pseudo_LiDAR> \n","https://github.com/mileyan/Pseudo_Lidar\n","\n","notebook maker : lee jiho (jiho264@naver.com)\n","\n","PSMNet 내 구형 torchvision 이슈로, PSMNet은 상위 버전을 가져와서, 코드를 일부 수정해야함.\n","https://github.com/JiaRenChang/PSMNet"]},{"cell_type":"markdown","metadata":{},"source":["정리\n","1. PSMNet 알고리즘으로 depth map 만듦 (.png >>> .npy)\n","2. depth map을 point cloud로 만듦 (.npy >>> .bin)\n","3. frustum pointnet 알고리즘으로 데이터 전처리 실행. 원본 .npg + pseudo lidar의 output인 .bin 이용. (.png + .bin >>> .pickle)\n","4. frustum의 viewer code 들로 객체 감지와, dataset 전체의 객체 감지 통계 확인가능."]},{"cell_type":"markdown","metadata":{},"source":["# 1. Overview\n","We provide the guidance and codes to train stereo depth estimator and 3D object detector using the KITTI object detection benchmark. We also provide our pre-trained models."]},{"cell_type":"markdown","metadata":{},"source":["# 2. Usage"]},{"cell_type":"markdown","metadata":{},"source":["## 2.1 Dependencies\n","- Python 3.5+\n","- numpy, scikit-learn, scipy\n","- KITTI 3D object detection dataset\n","- CUDA, Torch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''\n","1. cuda 11.3.1 install\n","    wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin\n","    sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600\n","    wget https://developer.download.nvidia.com/compute/cuda/11.3.1/local_installers/cuda-repo-ubuntu2004-11-3-local_11.3.1-465.19.01-1_amd64.deb\n","    sudo dpkg -i cuda-repo-ubuntu2004-11-3-local_11.3.1-465.19.01-1_amd64.deb\n","    sudo apt-key add /var/cuda-repo-ubuntu2004-11-3-local/7fa2af80.pub\n","    sudo apt-get update\n","    sudo apt-get -y install cuda\n","2. maybe have dpkg error (1) \n","    cd /var/lib/dpkg\n","    sudo rm -r info\n","    sudo mkdir info\n","3. retry install CUDA\n","    sudo apt-get -y install cuda\n","4. .bach edit. \n","    sudo gedit /etc/bash.bashrc    \n","        >>> add to last line\n","        export PATH=$PATH:/usr/local/cuda-11.3/bin\n","        export CUDADIR=/usr/local/cuda-11.3\n","        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.3/lib64\n","    source /etc/bash.bashrc\n","5. check my driver and remove (if display resolution 640*480)\n","    dpkg -l | grep -i nvidia\n","    >>> ex) nvidia-driver-465\n","    sudo apt remove nvidia-driver-465\n","6. check and install \n","    ubuntu-drivers devices\n","    sudo apt install nvidia-driver-515\n","    sudo reboot\n","7. install pytorch 1.11.0+cu113 \n","    pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113\n","8. CHECK\n","    nvidia-smi \n","    >>> driver-version == 515, CUDA 11.7\n","    nvcc --version\n","    >>> CUDA 11.3.1\n","'''\n","# 꼭 CUDA & pytorch version 같은지 확인해야함.\n","import torch\n","print(torch.cuda.is_available())\n","print(torch.__version__)\n","# >>> Ture\n","# >>> 1.11.0+cu113\n","# VGA Driver의 CUDA와, CUDA Toolkit의 버전은 다를 수 있고, 전혀 상관 없다.\n","!nvidia-smi \n","!nvcc --version"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%pip install preprocess\n","%pip install scikit-learn\n","%pip install scikit-image\n","%pip install Pillow==9.2.0"]},{"cell_type":"markdown","metadata":{},"source":["## 2.2 Download the dataset"]},{"cell_type":"markdown","metadata":{},"source":["KITTI/object/\n","    \n","    train.txt\n","    val.txt\n","    test.txt \n","    \n","    training/\n","        calib/\n","        image_2/ #left image\n","        image_3/ #right image\n","        label_2/\n","        velodyne/ \n","\n","    testing/\n","        calib/\n","        image_2/\n","        image_3/\n","        velodyne/\n","\n","http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d\n","\n","https://github.com/charlesq34/frustum-pointnets/tree/master/kitti/image_sets\n","\n","프로젝트/Pseudo_lidar-marter/KITTI/...\n","\n","프로젝트/frustum_pointnet/...  의 형태로 디렉터리를 맞춘다."]},{"cell_type":"markdown","metadata":{},"source":["## 2.3 Generate ground-truth image disparities\n"]},{"cell_type":"markdown","metadata":{},"source":["- velodyne의 .bin (point cloud data)를 .npy 로 바꿔줌.\n","- 실제 disparity를 구하는 부분"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''\n","issue 1 : module error\n","    1. edit to './preprocessing/generate_disp.py'\n","        line 5 : import scipy.misc as ssc >> import skimage\n","        line 64 : ssc.imread >> skimage.io.imread\n","      \n","issue 2 : ubuntu system off\n","    1. add to './preprocessing/generate_disp.py' line 58-59;\n","        if predix < '______':\n","            continue\n","        ('______' is last saved disparity file. if last file is 001515.npy >> '_______' == 001500)\n","    2. save and retry\n","'''\n","!python3 ./preprocessing/generate_disp.py --data_path ./KITTI/object/training/ --split_file ./KITTI/object/train.txt "]},{"cell_type":"markdown","metadata":{},"source":["## 2.4. Train the stereo model (Skip!!)\n"]},{"cell_type":"markdown","metadata":{},"source":["- KITTI Dataset을 이용해서 학습시킴.\n","- 오래걸리니, github에서 finetune_300.tar 파일을 다운받자."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''\n","if can't find module file\n","    1. $ gedit ~/.bashrc\n","    2. add to last line \n","        export PYTHONPATH=\"${PYTHONPATH}:/home/(UESRNAME)/plv1/psmnet/models/\"\n","    3. $ source ~/.bashrc\n","    4. reboot to vscode and THIS FILE\n","    5. CHECK\n","        import sys\n","        sys.path >> {..., '/home/(UESRNAME)/plv1/psmnet/models/', ...}\n","\n","issue 2.\n","    /home/jiho/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: \n","    UserWarning: This DataLoader will create 14 worker processes in total. \n","    Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create.\n","    Please be aware that excessive worker creation might get DataLoader running slow or even freeze, \n","    lower the worker number to avoid potential slowness/freeze if necessary.\n","\n","    File \"/home/jiho/.local/lib/python3.8/site-packages/torch/_utils.py\", line 457, in reraise\n","      raise exception\n","    AttributeError: Caught AttributeError in DataLoader worker process 0.\n","\n","    File \"/home/jiho/plv1/psmnet/dataloader/KITTILoader_dataset3d.py\", line 58, in __getitem__\n","      processed = preprocess.get_transform(augment=False)\n","    AttributeError: module 'preprocess' has no attribute 'get_transform'\n","    >>> add to python PATH\n","\n","이거 해결 못했는데, 그냥 github 페이지 뒤지니까 3,712개 모델 학습된거 찾음. 그냥 파일 경로따라서 넣어주면됨\n","https://drive.google.com/file/d/1sWjsIO9Fuy92wT3gLkHF3PA7SP8QZBzu/view\n","./psmnet/kitti_3d/finetune_300.tar\n","'''\n","# 이거 우리 수준에서 절대 불가\n","# pseudo lidar github에 있는 psmnet 폴더에 있는 finetune_3d.py를 실행시켜야함. 현재의 finetune.py로는 불가\n","#!python3 ./psmnet/finetune_3d.py --maxdisp 192 --model stackhourglass --datapath ./KITTI/object/training/ --split_file ./KITTI/object/train.txt --epochs 300 --lr_scale 50 --loadmodel ./pretrained_sceneflow.tar --savemodel ./psmnet/kitti_3d/ --btrain 6"]},{"cell_type":"markdown","metadata":{},"source":["## 2.5.1 Predict the point clouds - Predict the disparities.\n"]},{"cell_type":"markdown","metadata":{},"source":["- training / .png >>> .npy(or .png)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/jiho/plv1\n","Number of model parameters: 5224768\n","time = 0.63\n","000000.png\n","time = 0.49\n","000001.png\n","time = 0.50\n","000002.png\n","time = 0.49\n","000003.png\n","time = 0.48\n","000004.png\n","^C\n","Traceback (most recent call last):\n","  File \"./psmnet/submission.py\", line 116, in <module>\n","    main()\n","  File \"./psmnet/submission.py\", line 104, in main\n","    pred_disp = test(imgL,imgR)\n","  File \"./psmnet/submission.py\", line 70, in test\n","    output = model(imgL,imgR)\n","  File \"/home/jiho/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/jiho/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 166, in forward\n","    return self.module(*inputs[0], **kwargs[0])\n","  File \"/home/jiho/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/jiho/plv1/psmnet/models/stackhourglass.py\", line 110, in forward\n","    cost = Variable(torch.FloatTensor(refimg_fea.size()[0], refimg_fea.size()[1]*2, self.maxdisp//4,  refimg_fea.size()[2],  refimg_fea.size()[3]).zero_()).cuda()\n","KeyboardInterrupt\n"]}],"source":["'''\n","issue 1. \n","    TypeError: new(): argument 'size' must be tuple of ints, but found element of type float at pos 3\n","    >>> float을 int로 형변환만 해주면 됨.\n","\n","    \"./psmnet/models/stackhourglass.py\"\n","    torch.FloatTensor(refimg_fea.size()[0], refimg_fea.size()[1] * 2, self.maxdisp / 4, refimg_fea.size()[2], refimg_fea.size()[3]).zero_()).cuda()\n","    >>> torch.FloatTensor(refimg_fea.size()[0], refimg_fea.size()[1] * 2, int(self.maxdisp / 4), refimg_fea.size()[2], refimg_fea.size()[3]).zero_()).cuda()\n","    for i in range(self.maxdisp / 4):\n","    >>> for i in range(int(self.maxdisp / 4)):\n","\n","issue 2. \n","    RuntimeError: CUDA out of memory 부족 \n","    >>> 똥컴탓\n","\n","issue 3.\n","    UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","    warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n","    >>> upsample을 interpolate로 변경해주기.\n","\n","finetune_300.tar 파일을 다운받고 ./psmnet에 넣어야 학습된 신경망 가져오기 가능.\n","\n","두 psmnet 버전에 따라, input args가 다르다. \n","1. 후 버전의 코드를 조금 수정해서, 변환 파일 저장 위치를 바꾼다.\n","2. submission.py의 output이 .png가 아니라 .npy가 되도록 기존버전에서 저장과 진행상황 알려주는 코드를 가져온다.\n","        np.save(args.save_path+'/'+test_left_img[inx].split('/')[-1][:-4], img)\n","        print(test_left_img[inx].split('/')[-1])\n","        >>> def main(): 의 맨 마지막에 코드 추가.\n","3. psmnet/dataloader/KITTI_submission_loader.py에서 line 24에 'image = sorted(image)'를 추가한다.\n","'''\n","%cd /home/jiho/plv1\n","!python3 ./psmnet/submission.py --loadmodel ./psmnet/finetune_300.tar --datapath ./KITTI/object/training/ --save_path ./KITTI/object/training/predict_disparity"]},{"cell_type":"markdown","metadata":{},"source":["- testing / .png >>> .npy(or .png)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python3 ./psmnet/submission.py --loadmodel ./psmnet/kitti_3d/finetune_300.tar --datapath ./KITTI/object/testing/ --save_path ./KITTI/object/testing/predict_disparity"]},{"cell_type":"markdown","metadata":{},"source":["## 2.5.2 Predict the point clouds - Convert the disparities to point clouds."]},{"cell_type":"markdown","metadata":{},"source":["- training / .npy(or .png) >>> .bin"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Finish Depth 000000\n","Finish Depth 000001\n","Finish Depth 000002\n","Finish Depth 000003\n","Finish Depth 000004\n"]}],"source":["!python3 ./preprocessing/generate_lidar.py --calib_dir ./KITTI/object/training/calib/ --save_dir ./KITTI/object/training/pseudo-lidar_velodyne/ --disparity_dir ./KITTI/object/training/predict_disparity --max_high 1"]},{"cell_type":"markdown","metadata":{},"source":["- testing  / .npy(or .png) >>> .bin\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python3 ./preprocessing/generate_lidar.py --calib_dir ./KITTI/object/testing/calib/ --save_dir ./KITTI/object/testing/pseudo-lidar_velodyne/ --disparity_dir ./KITTI/object/testing/predict_disparity --max_high 1"]},{"cell_type":"markdown","metadata":{},"source":["- 2.5.1과 2.5.2의 과정을 거치면 .png >>> .npy >>> .bin 파일이 된다.\n","- 이 파일들은 바로 밑에서 path를 수정하고 실행해서 확인"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/jiho/.local/lib/python3.8/site-packages/pythreejs/traits.py:203: UserWarning: 64-bit data types not supported for WebGL data, casting to 32-bit.\n","  warnings.warn('64-bit data types not supported for WebGL '\n","/home/jiho/.local/lib/python3.8/site-packages/jupyter_client/session.py:718: UserWarning: Message serialization failed with:\n","Out of range float values are not JSON compliant\n","Supporting this message is deprecated in jupyter-client 7, please make sure your message is JSON-compliant\n","  content = self.pack(content)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a9836f7142284b54a94d46254ded22c0","version_major":2,"version_minor":0},"text/plain":["Renderer(camera=PerspectiveCamera(aspect=1.6, fov=90.0, position=(11.347519311071496, 18.742071965291153, 0.62…"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ded6294d3e574b558202de46585ab78f","version_major":2,"version_minor":0},"text/plain":["HBox(children=(Label(value='Point size:'), FloatSlider(value=0.02, max=0.2, step=0.0002), Label(value='Backgro…"]},"metadata":{},"output_type":"display_data"}],"source":["from pyntcloud import PyntCloud\n","import pandas as pd\n","import os\n","import numpy as np\n","import PIL.Image as Image\n","%matplotlib inline\n","def load_velo_scan(velo_filename):\n","    scan = np.fromfile(velo_filename, dtype=np.float32)\n","    scan = scan.reshape((-1, 4))\n","    return scan\n","def paint_points(points, color=[192,0,0]):\n","    # color = [r, g, b]\n","    color = np.array([color])\n","    new_pts = np.zeros([points.shape[0],6])\n","    new_pts[:,:3] = points\n","    new_pts[:, 3:] = new_pts[:, 3:] + color\n","    return new_pts\n","# 여기를 보고싶은 .bin 파일의 경로로 수정.\n","path = '/home/jiho/plv1/KITTI/object/training/pseudo-lidar_velodyne/000000.bin'\n","points = load_velo_scan(path)[:,:3]\n","pd_points = pd.DataFrame(paint_points(points), columns=['x','y','z','red','green','blue'])\n","cloud = PyntCloud(pd_points)\n","cloud.plot(initial_point_size=0.02)"]},{"cell_type":"markdown","metadata":{},"source":["## 2.6 Generate ground plane"]},{"cell_type":"markdown","metadata":{},"source":["- training"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-------------  000000\n","[[ 0.00692796 -0.9999722  -0.00275783]\n"," [-0.00116298  0.00274984 -0.9999955 ]\n"," [ 0.9999753   0.00693114 -0.0011439 ]\n"," [ 0.33219371 -0.02210627 -0.06171977]]\n","12\n","[-0.00810649 -0.9997002  -0.02310421]\n","1.6868425923156682\n","-------------  000004\n","[[ 7.53374500e-03 -9.99971400e-01 -6.16602000e-04]\n"," [ 1.48024900e-02  7.28073300e-04 -9.99890200e-01]\n"," [ 9.99862100e-01  7.52379000e-03  1.48075500e-02]\n"," [ 2.72903452e-01 -1.96926567e-03 -7.22859051e-02]]\n","12\n","[-0.00168079 -0.99999426  0.00294331]\n","1.6519988373158707\n","-------------  000003\n","[[ 7.53374500e-03 -9.99971400e-01 -6.16602000e-04]\n"," [ 1.48024900e-02  7.28073300e-04 -9.99890200e-01]\n"," [ 9.99862100e-01  7.52379000e-03  1.48075500e-02]\n"," [ 2.72903452e-01 -1.96926567e-03 -7.22859051e-02]]\n","12\n","[-0.06423007 -0.99774134  0.01966527]\n","1.5695283375878053\n","-------------  000001\n","[[ 7.53374500e-03 -9.99971400e-01 -6.16602000e-04]\n"," [ 1.48024900e-02  7.28073300e-04 -9.99890200e-01]\n"," [ 9.99862100e-01  7.52379000e-03  1.48075500e-02]\n"," [ 2.72903452e-01 -1.96926567e-03 -7.22859051e-02]]\n","12\n","[-8.62849151e-03 -9.99962771e-01  6.99344490e-05]\n","1.6604458356884892\n","-------------  000002\n","[[ 7.53374500e-03 -9.99971400e-01 -6.16602000e-04]\n"," [ 1.48024900e-02  7.28073300e-04 -9.99890200e-01]\n"," [ 9.99862100e-01  7.52379000e-03  1.48075500e-02]\n"," [ 2.72903452e-01 -1.96926567e-03 -7.22859051e-02]]\n","12\n","[-0.0102018  -0.99968274  0.02302935]\n","1.5536562240324523\n"]}],"source":["!python3 ./preprocessing/kitti_process_RANSAC.py --calib ./KITTI/object/training/calib/ --lidar_dir  ./KITTI/object/training/pseudo-lidar_velodyne/ --planes_dir ./KITTI/object/training/pseudo-lidar_planes/"]},{"cell_type":"markdown","metadata":{},"source":["- testing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python3 ./preprocessing/kitti_process_RANSAC.py --calib ./KITTI/object/testing/calib/ --lidar_dir  ./KITTI/object/testing/pseudo-lidar_velodyne/ --planes_dir ./KITTI/object/testing/pseudo-lidar_planes/"]},{"cell_type":"markdown","metadata":{},"source":["# 3. Object Detection models - Frustum-PointNets model"]},{"cell_type":"markdown","metadata":{},"source":["pip install mayavi\n","pip install opencv-python\n","sudo apt install libxcb-xinerama0-dev\n","pip install pyqt5"]},{"cell_type":"markdown","metadata":{},"source":["## Usage"]},{"cell_type":"markdown","metadata":{},"source":["### Setting Check - 객체 라벨링 확인할 수 있음"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/jiho/my\n","multiple 'pyface.toolkits' plugins found for toolkit 'qt4': pyface.ui.qt4.init, pyface.ui.qt4.init\n","^C\n","Traceback (most recent call last):\n","  File \"kitti/kitti_object.py\", line 222, in <module>\n","    import mayavi.mlab as mlab\n","  File \"/usr/local/lib/python3.8/dist-packages/mayavi/mlab.py\", line 34, in <module>\n","    from .tools.helper_functions import contour3d, test_contour3d, \\\n","  File \"/home/jiho/.local/lib/python3.8/site-packages/shiboken2/files.dir/shibokensupport/__feature__.py\", line 142, in _import\n","    return original_import(name, *args, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/mayavi/tools/helper_functions.py\", line 16, in <module>\n","    from .modules import VectorsFactory, StreamlineFactory, GlyphFactory, \\\n","  File \"/home/jiho/.local/lib/python3.8/site-packages/shiboken2/files.dir/shibokensupport/__feature__.py\", line 142, in _import\n","    return original_import(name, *args, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/mayavi/tools/modules.py\", line 23, in <module>\n","    import mayavi.modules.api as modules\n","  File \"/home/jiho/.local/lib/python3.8/site-packages/shiboken2/files.dir/shibokensupport/__feature__.py\", line 142, in _import\n","    return original_import(name, *args, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/mayavi/modules/api.py\", line 34, in <module>\n","    from .warp_vector_cut_plane import WarpVectorCutPlane\n","  File \"/home/jiho/.local/lib/python3.8/site-packages/shiboken2/files.dir/shibokensupport/__feature__.py\", line 142, in _import\n","    return original_import(name, *args, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/mayavi/modules/warp_vector_cut_plane.py\", line 30, in <module>\n","    class WarpVectorCutPlane(Module):\n","  File \"/usr/local/lib/python3.8/dist-packages/mayavi/modules/warp_vector_cut_plane.py\", line 66, in WarpVectorCutPlane\n","    View(Item('scale_factor')))),\n","  File \"/home/jiho/.local/lib/python3.8/site-packages/traitsui/item.py\", line 284, in __init__\n","    value = self._split(\"object\", value, \".\", str.rfind, 0, 1)\n","  File \"/home/jiho/.local/lib/python3.8/site-packages/traitsui/view_element.py\", line 113, in _split\n","    col = finder(value, char)\n","KeyboardInterrupt\n"]}],"source":["'''\n","여기서부터는 frustum_pointnet dir로 이동해서 실행\n","issue 1.\n","    File \"/home/jiho/my/kitti/kitti_util.py\", line 381, in draw_projected_box3d\n","    cv2.line(image, (qs[i,0],qs[i,1]), (qs[j,0],qs[j,1]), color, thickness, cv2.CV_AA)\n","    solution : CV_AA >>> LINE_AA\n","\n","한 쌍의 .png로부터 얻은 모든 정보를 볼 수 있다.\n","'''\n","%cd /home/jiho/my\n","!python3 kitti/kitti_object.py"]},{"cell_type":"markdown","metadata":{},"source":["### Prepare Training Data\n"]},{"cell_type":"markdown","metadata":{},"source":["- .bin, .png 를 이용하여, python에서 읽을 수 있는 pickle file로 만든다."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''\n","issue 1.\n","  File \"kitti/prepare_data.py\", line 18, in <module>\n","  import cPickle as pickle\n","  >>> import pickle\n","\n","but, 4.7GB짜리 .plckle file set 옮겨놓으면 생략가능.\n","\n","issue 2.\n","  prepare_data.py; line 46.\n","  from viz_util import *\n","  >>> add PATH /home/jiho/my/mayavi\n","\n","애초에 pseudo lidar가 올바르게 만들어지지 않아서, pickle의 데이터 용량도 작았던 것임.\n","라이다 제대로 변환해서, 피클만 똑바로 뽑아내면 이후 확인작업 수월히 할 수 있음.\n","pickle 데이터는 lidar에 영향받음.\n","\n","원래 pseudo_lidar-velodyne 폴더를 velodyne으로 이름 변경해야한다.\n","테스트 시, 원본 KITTI dataset과 섞이지 않게 주의.\n","'''\n","%cd /home/jiho/my\n","#!sh scripts/command_prep_data.sh\n","!python3 kitti/prepare_data.py --gen_train\n","#!python3 kitti/prepare_data.py --gen_val\n","#!python3 kitti/prepare_data.py --gen_val_rgb_detection\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''\n","- frustum_carpedcyc_train_detection.pickle의 감지된 객체 2만 여개 항목 틀어줌.\n","- 터미널에서 실행 후, 결과 나온 뒤 엔터 누르면 다음 출력으로 넘어감.\n","- 2d, 3d, pseudo lidar 등 여러 정보 확인 가능.\n","'''\n","!python3 ./train/provider.py"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python3 ./kitti/prepare_data.py --demo"]},{"cell_type":"markdown","metadata":{},"source":["출력 불가 시 : pyqt5 9.x버전쓰면안됨. 개발자들이 버전 7에서 삽질함. 6.x로 깔면됨\n","\n","이 부분 여기 정확하지 않아서 수정예정"]},{"cell_type":"markdown","metadata":{},"source":["### Frustum PointNet 훈련\n"]},{"cell_type":"markdown","metadata":{},"source":["- Frustum Pointnet의 딥러닝 과정.\n","- 이미 훈련 시킨 모델파일 제공함"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#!CUDA_VISIBLE_DEVICES=0 sh scripts/command_train_v1.sh\n","#!CUDA_VISIBLE_DEVICES=0 python3 train/train.py --gpu 0 --model frustum_pointnets_v1 --log_dir train/log_v1 --num_point 1024 --max_epoch 201 --batch_size 32 --decay_step 800000 --decay_rate 0.5"]},{"cell_type":"markdown","metadata":{},"source":["NEW: We have also prepared some pretrained snapshots for both the v1 and v2 models. You can find them HERE (40MB) -- to support evaluation script, you just need to unzip the file and move the log_* folders to the train folder.\n","\n","frustum github에서 파일받고, log_v1, log_v2 총 40MB 옮기면 됨."]},{"cell_type":"markdown","metadata":{},"source":["### 평가\n","- Frustum pointnet 알고리즘이 차량, 보행자, 자전거 등을 얼마나 감지했는지 통계냄.\n","- pickle file을 바탕으로 통계를 작성하기 때문에, pickle file 생성에 예민하다."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''\n","issue 1.\n","    train/test.py\n","    line 16 \n","    import cPickle as pickle\n","    >> import pickle\n","issue 2.\n","    PATH : model_util.py \n","    \n","issue 3.\n"," File \"train/test.py\", line 231\n","    batch_output, batch_center_pred, \\\n","                                     ^\n","TabError: inconsistent use of tabs and spaces in indentation\n","\n","issue 4.\n"," File \"/home/jiho/my/train/provider.py\", line 8, in <module>\n","    import cPickle as pickle\n","    \n","issue 5.\n","Traceback (most recent call last):\n","  File \"train/test.py\", line 48, in <module>\n","    TEST_DATASET = provider.FrustumDataset(npoints=NUM_POINT, split='val',\n","  File \"/home/jiho/my/train/provider.py\", line 129, in __init__\n","    self.box2d_list = pickle.load(fp)\n","    UnicodeDecodeError: 'ascii' codec can't decode byte 0x85 in position 6: ordinal not in range(128)\n",">> 피클파일 직접 만들기\n","\n","issue 6. ===============텐서 수정\n","  tf.placeholder 작동안함\n","  >> test.py, model_uilt.py 안에서 모두 tf.compat.v1.placeholder로 변경\n","\n","issue 7. ===============텐서 수정\n","  File \"/home/jiho/my/models/frustum_pointnets_v1.py\", line 34, in get_instance_seg_v1_net\n","    batch_size = point_cloud.get_shape()[0].value\n","AttributeError: 'int' object has no attribute 'value'\n","\n","34,35 줄     batch_size = point_cloud.get_shape()[0]\n","    num_point = point_cloud.get_shape()[1]\n","    그냥 .value 지워버림\n","\n","issue 8.    \n","models/tf_util.py; line 155\n","with tf.compat.v1.variable_scope(scope) as sc:\n","\n","9.\n","File \"/home/jiho/my/models/tf_util.py\", line 527, in batch_norm_template\n","    return tf.contrib.layers.batch_norm(inputs, \n","AttributeError: module 'tensorflow' has no attribute 'contrib'\n","\n","    return tf.layers.batch_norm(inputs, \n","\n","!pip install tf-slim\n","import tf_slim as slim\n","  return slim.batch_norm(inputs, \n","으로 변경\n","\n","10.\n","packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n","    raise e.with_traceback(filtered_tb) from None\n","  File \"/home/jiho/my/models/tf_util.py\", line 614, in <lambda>\n","    true_fn=lambda: tf.nn.dropout(inputs, noise_shape, rate=1 - (keep_prob)),\n","TypeError: Got multiple values for argument 'rate'\n","\n","tensor 2로 올리면서 코드가 꼬임. 원본을 가져다 붙임.\n","    outputs = tf.cond(is_training,\n","                      lambda: tf.nn.dropout(inputs, keep_prob, noise_shape),\n","                      lambda: inputs)\n","11.\n","model_util.py 에서 value 삭제\n","\n","12.\n","2022-08-07 19:04:21.478824: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] \n","Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: \n","No such file or directory; \n","\n","install cudnn \n","Local Installer for Ubuntu20.04 x86_64 (Deb)\n","https://developer.nvidia.com/rdp/cudnn-download\n","\n","13.\n","model_util.py; line65\n","  indices = tf.py_function(mask_to_indices, [mask], tf.int32)  \n","  텐서 버전에 맞는 문법으로 고치기\n","\n","14. \n","batch idx: 792\n","batch idx: 793\n","Traceback (most recent call last):\n","  File \"train/test.py\", line 355, in <module>\n","    test_from_rgb_detection(FLAGS.output+'.pickle', FLAGS.output)\n","  File \"train/test.py\", line 234, in test_from_rgb_detection\n","    inference(sess, ops, batch_data_to_feed,\n","  File \"train/test.py\", line 113, in inference\n","    for i in range(num_batches):\n","TypeError: 'float' object cannot be interpreted as an integer\n",">>>     for i in range(int(num_batches)):\n","\n","15. \n","  File \"train/test.py\", line 114, in inference\n","    ops['pointclouds_pl']: pc[i*batch_size:(i+1)*batch_size,...],\n","TypeError: slice indices must be integers or None or have an __index__ method\n","\n","    num_batches = pc.shape[0]/batch_size\n",">>>     num_batches = int(pc.shape[0]/batch_size)\n","\n","16. install cudnn for CUDA 11.3 in ubuntu 20.04\n","  https://lapina.tistory.com/137\n","\n","  successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","  !cat /sys/bus/pci/devices/0000\\:01\\:00.0/numa_node\n","  >>> -1\n","  !echo 0 | sudo tee -a /sys/bus/pci/devices/0000\\:01\\:00.0/numa_node\n","  >>> 0\n","  !cat /sys/bus/pci/devices/0000\\:01\\:00.0/numa_node\n","  >>> 0\n","\n","17. edit batch size\n","  under 16(deflaut == 32)\n","\n","18. provider.py\n","    싹 다 바꾸기\n","    self.~~~~ = pickle.load(fp)\n","    >>> self.~~~~ = pickle.load(fp,encoding = 'ISO-8859-1')\n","'''\n","#%rm -r ./train/detection_results_v1/plot\n","#!CUDA_VISIBLE_DEVICES=0 sh scripts/command_test_v1.sh\n","!CUDA_VISIBLE_DEVICES=0 python3 train/test.py --gpu 0 --num_point 1024 --model_path train/log_v1/model.ckpt --batch_size 32 --output train/detection_results_v1 --data_path kitti/frustum_carpedcyc_val_rgb_detection.pickle --from_rgb_detection --idx_path kitti/image_sets/val.txt --from_rgb_detection"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''\n","apt-get install texlive-extra-utils\n","apt-get install gnuplot\n","apt-get install ghostscript\n","\n","'''\n","!train/kitti_eval/evaluate_object_3d_offline dataset/KITTI/object/training/label_2/ train/detection_results_v1"]},{"cell_type":"markdown","metadata":{},"source":["### Prepare Training \n","- frustum pointnet의 탐지 결과에 관한 readme 파일\n","\n","In this step we convert original KITTI data to organized formats for training our Frustum PointNets. <b>NEW:</b> You can also directly download the prepared data files <a href=\"https://shapenet.cs.stanford.edu/media/frustum_data.zip\" target=\"_blank\">HERE (960MB)</a> -- to support training and evaluation, just unzip the file and move the `*.pickle` files to the `kitti` folder.\n","\n","Firstly, you need to download the <a href=\"http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d\" target=\"_blank\">KITTI 3D object detection dataset</a>, including left color images, Velodyne point clouds, camera calibration matrices, and training labels. Make sure the KITTI data is organized as required in `dataset/README.md`. You can run `python kitti/kitti_object.py` to see whether data is downloaded and stored properly. If everything is fine, you should see image and 3D point cloud visualizations of the data. \n","\n","Then to prepare the data, simply run: (warning: this step will generate around 4.7GB data as pickle files)\n","\n","    sh scripts/command_prep_data.sh\n","\n","Basically, during this process, we are extracting frustum point clouds along with ground truth labels from the original KITTI data, based on both ground truth 2D bounding boxes and boxes from a 2D object detector. We will do the extraction for the train (`kitti/image_sets/train.txt`) and validation set (`kitti/image_sets/val.txt`) using ground truth 2D boxes, and also extract data from validation set with predicted 2D boxes (`kitti/rgb_detections/rgb_detection_val.txt`).\n","\n","You can check `kitti/prepare_data.py` for more details, and run `python kitti/prepare_data.py --demo` to visualize the steps in data preparation.\n","\n","After the command executes, you should see three newly generated data files under the `kitti` folder. You can run `python train/provider.py` to visualize the training data (frustum point clouds and 3D bounding box labels, in rect camera coordinate)."]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"name":"plv2.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
